# ğŸš€ DistributedAttention

**DistributedAttention** is a high-performance Transformer model implemented from scratch, optimized with various parallel computing techniques including OpenMP, MPI, and CUDA.

## ğŸ“Œ Project Status 
- âœ… **Baseline Transformer & Profiling** - Completed
- âœ… **OpenMP Optimization** - In Progress
- â³ **MPI Optimization** - In Progress
- â³ **CUDA Optimization** - In Progress

---
## âœ¨ Features

ğŸ”¹ **Baseline Transformer**: Implements a standard Transformer model from scratch.<br>
ğŸ”¹ **OpenMP Optimization**: Parallelizes computations using OpenMP. âš¡<br>
ğŸ”¹ **MPI Optimization**: Distributes computation across multiple nodes with MPI. ğŸŒ<br>
ğŸ”¹ **CUDA Optimization**: Speeds up computations using GPU acceleration. ğŸ®<br>

---
## ğŸ“– Overview
This project provides a highly optimized implementation of **Multi-Head Self-Attention (MHA)**, a key component of Transformer models, for **GPU architectures**. Using CUDA, the project explores and evaluates two parallelization strategies:
- **Horizontal parallelization** using native C++ threads.
- **Vertical parallelization** using CUDA streams and events.

The goal is to **achieve substantial speedups** for Transformer workloads, which are widely used in **Natural Language Processing (NLP) and Computer Vision (CV) tasks**.

---
## ğŸ“š Background
Transformer models and their multi-head attention mechanisms have revolutionized NLP and beyond. However, their **computational demands are significant**.

This project focuses on **Multi-Head Self-Attention** and explores how different **parallelization strategies affect performance**. We compare three approaches:
1. **Baseline CPU Implementation**: A naive, single-threaded C++ version for reference.
2. **MPI + OpenMP Parallelization**:
   - **MPI** distributes workloads across compute nodes.
   - **OpenMP** parallelizes computations within each node.
3. **CUDA Acceleration**:
   - Implements key operations on GPUs.
   - Uses **CUDA streams** to parallelize computations across multiple heads.

By comparing these approaches, this project highlights **trade-offs in complexity, scalability, and speedup** for different optimization strategies.

---

![Transformer Architecture](images/TransformerArchitecture.png)

---
## ğŸ›  Implementation Details
A single head attention kernel can be described as:

$$
Softmax\left(\frac{Q \times K^T}{\sqrt{d_k}}\right) \times V
$$

The Multi Head Attention Kernel includes three core operations:

- General Matrix Multiplication (GEMM)
- Softmax
- Transpose

The core of a multi head attention kernel is the single head attention kernel. The chronological order of a single head attention kernel can be described as:

- $X * W_Q \rightarrow Q(Q$ calculation $)$
- $X * W_K \rightarrow K$ (K calculation)
- $X * W_V \rightarrow V(V$ calculation $)$
- $Q * K^T\left(K^T\right.$ calculation $)$
- Softmax $\left(\frac{Q * K^T}{\sqrt{d_k}}\right)$
- Softmax $\left(\frac{Q * K^T}{\sqrt{d_k}}\right) \times V( Softmax * Vcalculation )$

---
### ğŸš€ Parallelization Strategy
**Task-Level Parallelism (MPI)**:
- We assign **8 attention heads per MPI process** to distribute workloads across compute nodes.

**Data-Level Parallelism (OpenMP)**:
- **Each matrix operation** (e.g., `Q Ã— K^T`, softmax, `Scores Ã— V`) involves data-parallel computations.
- OpenMP is used to parallelize matrix operations, distributing **matrix rows** across CPU threads.

**CUDA GPU Acceleration**:
- Implement and optimize key operations on GPUs to exploit **massive parallelism**.
- Use **CUDA streams** to parallelize computations across multiple heads.

---
## ğŸ›  Getting Started

### ğŸ“‹ Prerequisites
Ensure you have the following installed:
- ğŸ–¥ **C++ Compiler** (GCC/Clang)
- ğŸ **Python 3.x**
- ğŸ”— **OpenMP** (For multi-threading)
- ğŸ”— **MPI** (For distributed computing)
- ğŸ”— **CUDA** (For GPU acceleration)

### ğŸ“¥ Installation

1ï¸âƒ£ Clone the repository:
```sh
 git clone https://github.com/yourusername/DistributedAttention.git
 cd DistributedAttention
```

2ï¸âƒ£ Compile the C++ Transformer model:
```sh
 g++ -fopenmp -o transformer transformer.cpp
```

3ï¸âƒ£ Run the Transformer model:
```sh
 ./transformer
```

---
## ğŸ§ª Running Tests & Profiling
To analyze performance, generate profiling data using `gprof2dot.py`:

```sh
python baseline/gprof2dot.py -f profile.txt -o output.dot
```

Convert the profiling data into an image:
```sh
dot -Tpng output.dot -o output.png
```

---
## ğŸ“œ Roadmap

âœ… Implement baseline Transformer ğŸ”¹ **Done**  
âœ… OpenMP parallelization ğŸ”¹ **In Progress**  
ğŸ”œ MPI distributed optimization ğŸ”¹ **In Progress**  
ğŸ”œ CUDA acceleration ğŸ”¹ **In Progress**  

---
## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

ğŸ“Œ *Contributions are welcome! Feel free to fork, raise issues, and submit PRs!* ğŸš€
